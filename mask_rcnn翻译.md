# Mask RCNN

# 概要

​	我们提出了一个概念上简单，灵活，通用的对象实例分割框架。我们的方法有效地检测图像中的对象，同时为每个实例生成高质量的分割掩码。该方法称为Mask R-CNN，通过添加一个分支来扩展faster R-CNN，该分支用于与现有分支并行地预测对象掩码以进行边界框识别。 Mask R-CNN易于训练，只需很少的开销即可以更快的速度增加R-CNN，运行速度为每秒5张图片。此外，Mask R-CNN很容易推广到其他任务，例如，允许我们在同一框架中估计人体姿势。我们在COCO挑战的所有三个项目中展示了最佳结果，包括实例分割，边界框对象检测和人员关键点检测。没有花哨的功能，Mask R-CNN在每项任务中都优于所有现有的单一模型，包括COCO 2016挑战赛冠军。我们希望我们简单有效的方法将成为一个坚实的基线，并有助于简化未来在实例级别识别方面的研究.

# 1.简介

​	视觉社区在很短的时间内迅速改进了对象检测和语义分割结果。 在很大程度上，这些进步是由强大的基线系统驱动的，例如fast/faster R-CNN 和完全卷积网络（FCN）框架分别用于对象检测和语义分割。 这些方法在概念上是直观的，并提供灵活性和稳健性，以及快速的训练和测试时间。 我们在这项工作中的目标是为实例分割提供一个比较好的基础框架。
​	实例分割具有挑战性，因为它需要正确检测图像中的所有对象，同时还要精确地分割每个实例。 因此，它结合了对象检测的经典计算机视觉任务中的元素，其目标是对各个对象进行分类，并使用边界框和语义分割对每个对象进行定位。	语义分割目标是在不区分对象实例的情况下将每个像素分类为一组固定的类别.鉴于此，人们可能期望需要一种复杂的方法来获得良好的结果。 然而，我们发现了一个令人惊讶的简单，灵活和快速的系统可以超越先前的最新实例分割结果。

​	我们的方法称为Mask R-CNN，通过在每个感兴趣区域（RoI）上添加用于预测分割掩模的分支来扩展faster R-CNN，与用于分类和边界框回归的现有分支并行 （图1）。 掩模分支是应用于==每个RoI==的小FCN，以像素到像素的方式预测分割掩模。 基于faster rcnn, Mask R-CNN易于实现和训练，这有助于广泛的灵活架构设计。 此外，掩码分支仅增加了小的计算开销，从而实现了快速的系统。

​	![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-11-20_16-02-42.png)

​	原则上，mask R-CNN是faster R-CNN的直观扩展，但正确构建掩模分支对于获得良好结果非常关键。 最重要的是，faster R-CNN并非设计用于网络输入和输出之间的像素到像素对齐。 这一点在RoIPool是如何实现对实例的实际中心的操作，为特征提取执行粗略空间量化时最为明显。 为了解决这个错位，我们提出了一个简单的，无量化的层，称为RoIAlign，真正地保留了精确的空间位置。 尽管看似微小的变化，但RoIAlign的影响很大：它将掩模精度提高了10％到50％，在更严格的定位指标下显示出更大的收益。其次，我们发现把掩码和类预测分离是必不可少的：我们独立地预测每个类的二进制掩码，没有类之间的竞争，并依赖于网络的RoI分类分支来预测类别。相比之下，FCNs 通常执行逐个像素的多类别分类，结合了实例分割和分类，在我们的实验下，它的分割效果不佳。

​	没有什么花哨，Mask R-CNN在COCO实例分割任务上超越了之前所有最先进的单一模型结果，包括来自2016年竞赛获胜者的精心设计的参赛作品。作为副产品，我们的方法也擅长COCO对象检测任务。

​	我们的模型可以在GPU上以每帧大约200ms的速度运行，而COCO上的训练在单个8-GPU机器上需要一到两天。 我们相信快速训练和测试速度以及框架的灵活性和准确性将有利于并简化未来对实例分割的研究。
​        最后，我们通过COCO关键点数据集上的人体姿态估计任务展示了我们框架的普遍性。通过将每个关键点视为二进制的独热码掩码，通过微小的修改，mask R-CNN可以用来检测特定于实例的姿势。Mask R-CNN超越了2016年COCO关键点竞赛的胜利者，同时以5 fps的速度运行。 因此，MASK R-CNN可以看作用于实例级识别的灵活框架，并且可以容易地扩展到更复杂的任务。

​	 R-CNN：用于边界框对象检测的基于区域的CNN（R-CNN）方法是为了处理可管理数量的候选对象区域并对每个RoI分别独立的评估卷积网络。 R-CNN被扩展到通过RoI Pool在特征映射图上提取RoI，从而实现更快的速度和更高的准确性。faster rcnn利用rpn实现了注意力机制。faster R-CNN是灵活并且健壮的，并且可以进行许多后续改进。	

​	

# 3.掩盖R-CNN

​	掩码R-CNN在概念上很简单：faster R-CNN为每个候选对象提供两个输出，一个类标签和一个边界框偏移;为此，我们添加了第三个分支，它输出了对象掩码。因此，mask R-CNN是一种自然而有益的想法。但是额外的掩码输出与类和框输出不同，需要提取对象的更精细的空间layout。接下来，我们介绍Mask R-CNN的一些关键元素，包括像素到像素对齐，这是fast/faster R-CNN的主要缺失部分。

​	**faster R-CNN**：我们首先简要回顾一下faster R-CNN探测器。faster R-CNN包括两个阶段。第一阶段称为区域提议网络（RPN），提出候选对象边界框。第二阶段，实质上是fast R-CNN ，从每个候选框中使用RoIPool提取特征并执行分类和边界框回归。可以共享两个阶段使用的功能，以便更快地测试。

​	**掩码R-CNN**：掩码R-CNN采用相同的两阶段过程，具有相同的第一阶段（即RPN）。 在第二阶段，与预测类和边框偏移并行，Mask R-CNN也为每个RoI输出二进制掩码。我们的方法遵循faster R-CNN 的精神，应用边界框分类和回归 并行（结果很大程度上简化了原始R-CNN的多级流水线）。

​	正式地，在训练期间，我们将每个采样的RoI上的多任务损失定义为$L = L_{cls} + L_{box} + L_{mask}$。 分类损失Lcls和边界框损失Lbox与faster rcnn中定义的相同。 掩码分支对于每个RoI具有$Km^2$维输出，其编码分辨率为m×m的K个二进制掩码，每个K类对应一个。 为此我们应用每像素sigmoid，并将Lmask定义为平均二进制交叉熵损失。 对于与实际的标签类k相关联的RoI，Lmask仅在第k个掩模上定义（其他掩模输出不会导致损失）。
​        我们对Lmask的定义允许网络为每个类生成掩码，而不需要在类之间进行竞争; 我们依靠专用的分类分支来预测用于选择输出掩码的类标签。 这解耦了掩码和类预测。 这与将FCN应用于语义分割时的常规做法不同，语义分割通常使用每像素softmax和多元交叉熵损失。 在这种情况下，各类的mask会有竞争; 在我们的例子中，使用的是每个像素sigmoid和二进制损失。 我们通过实验证明，这种方法是良好实例分割结果的关键。

​	**掩码表示**：掩码编码输入对象的空间布局。 因此，与通过完全连接（fc）层不可避免地折叠成短输出矢量的类标签或边框偏移不同，提取掩模的空间结构可以自然地通过由卷积提供的像素到像素的对应区域来解决。

​	具体来说，我们使用FCN预测每个RoI的m×m掩模。 这允许掩模分支中的每个层保持显式的m×m对象空间布局，而不将其折叠成缺少空间维度的矢量表示。 与先前为掩模预测重新分类为fc层的方法不同，我们的完全卷积表示只需要更少的参数，并且如实验所证明的那样更加准确。
​	这种像素到像素的行为要求我们的RoI特征（它们本身就是小特征映射）能够很好地对齐，以真正地保持显式的每像素空间相关性。 这促使我们开发以下RoIAlign层，该层在掩模预测中起关键作用。

 ![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-11-20_17-23-45.png)



**RoIAlign：**RoIPool 是从每个RoI中提取小特征图（例如，7×7）的标准操作。 RoIPool首先将浮点数RoI量化为特征映射的离散粒度，然后将该量化的RoI细分为自身量化的空间区间，最后聚合每个区域覆盖的特征值（通常通过最大池化）。 例如，通过计算[x / 16]在连续坐标x上执行量化，其中16是特征图步幅、[·]是舍入操作; 同样地，当分成bins（例如，7×7）时也是执行量化。 这些量化引入了RoI和提取的特征之间的未对准。 虽然这可能不会影响对健壮性的分类，但它对预测像素精确掩模有很大的负面影响。

​	为了解决这个问题，我们提出了一个RoIAlign层来消除RoIPool的严格量化，正确地将提取的特征与输入对齐。我们提出的改变很简单：我们避免对RoI边界进行任何量化（即，我们使用x / 16而不是[x / 16]）。我们使用双线性插值来计算每个RoI bin中四个常规采样位置的输入要素的精确值，并汇总结果（使用最大值或平均值），详情请参见图3。我们注意到，只要不执行量化，结果就不会对精确采样位置或采样点数敏感。

**网络架构：**为了演示我们方法的一般性，我们使用多种架构实例化Mask R-CNN。为清楚起见，我们区分：（i）用于整个图像上的特征提取的卷积骨干架构，以及（ii）用于边界框识别（分类和回归）的网络head 和 单独应用的掩模预测每个RoI。

​	我们使用命名的网络深度特征来表示骨干架构。我们评估深度为50或101层的ResNet 和ResNeXt 网络。faster的R-CNN与ResNets 的实现从第4阶段的最终卷积层中提取了特征，我们称之为C4。例如，ResNet-50的这个主干由ResNet-50-C4表示。
我们还探讨了Lin等人最近提出的另一个更有效的骨干称为特征金字塔网络（FPN）。 FPN使用具有横向连接的自上而下架构，从单一尺度输入构建网内特征金字塔。带有FPN的faster rcnn根据比例从不同水平的特征金字塔中提取特征。使用ResNet-FPN骨干网络通过Mask R-CNN进行特征提取，可在精度和速度方面获得极佳的提升。

​	对于网络头部，我们密切关注以前工作中提出的体系结构，我们在其中添加了完全卷积层mask预测分支。具体来说，我们从ResNet 和FPN 中提取了特征 。详细信息如图4所示.vResNet-C4 backbone 网络的头部包括ResNet的第5阶段（即9层'res5'）。对于FPN，主干网已经包含res5层，因此允许使用更少过滤器的更高效的网络头部。

![](https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2018-11-20_17-25-02.png)



​	图4.头部架构：我们扩展了两个现有的更快的R-CNN头。左/右面板分别显示ResNet C4和FPN骨干的头部，其中添加了遮罩分支。
数字表示空间分辨率和通道。箭头表示conv，deconv或fc层（conv会保留空间维度，而deconv会增加它）。所有转换都是3×3，除了输出转换为1×1，deconvs是2×2，步长为2，我们在隐藏层中使用ReLU 。左：'res5'表示ResNet的第五阶段，为简单起见，我们对其进行了修改，使得第一轮转换为7×7 RoI，步幅为1（而不是[19]中的14×14 /步幅2）。右：'×4'表示四个连续转换的堆栈。



## 3.1 实施细节

我们根据现有的 fast/faster R-CNN工作设置超参数。尽管这些决策都是针对原始论文中的对象检测做出的，但我们发现我们的实例分割系统对它们是健壮的。

**训练**：与在Fast R-CNN中一样，如果与实际的边框对比，IoU至少为0.5 ，则认为RoI为正，否则为负。掩模损失值Lmask仅在正RoI上定义。掩模target是RoI与其相关的实际掩模之间的交集。

我们采用以图像为中心的训练。图像的大小调整使得它们的比例（较短边缘）为800像素。每个小批量每个GPU有2个图像，每个图像有N个采样的RoI，正负比为1：3。 C4主干的N为64，FPN的N为512。我们训练8个GPU（因此有效的小批量大小为16）进行160k次迭代，学习率为0.02，在120k迭代时减少10。我们使用0.0001的重量衰减和0.9的动量。使用ResNeXt，我们训练每个GPU有1个图像和相同的迭代次数，起始学习率为0.01。

RPN锚点跨越5个尺度和3个纵横比。对于本文中的每个实例，RPN和Mask R-CNN具有相同的主干，因此它们是可共享的。

**在测试时，**C4主干的推荐区域数量为300，FPN为1000。使用非最大抑制, 我们在这些推荐区域上运行预测边框回归。然后将掩模分支应用于最高得分的100个检测框。虽然这与训练中使用的并行计算不同，但它加速了推断过程并提高了准确性（由于使用更少，更准确的RoI）。掩码分支可以预测每个RoI的K个掩码，但是我们仅使用第k个掩码，其中k是分类分支的预测类。然后将m×m浮点掩码输出调整大小为RoI大小，并在阈值0.5处二进制化。

注意，由于我们仅在前100个检测框上计算掩模，因此掩模R-CNN为其更快的R-CNN对应物增加了小的开销（例如，在典型模型上为~20％）。













比如你弄了个目标检测的pipeline用了A, B, C，然后效果还不错，但你并不知道A, B, C各自到底起了多大的作用，可能B效率很低同时精度很好，也可能A和B彼此相互促进。
Ablation experiment就是用来告诉你或者读者整个流程里面的关键部分到底起了多大作用，就像Ross将RPN换成SS进行对比实验，以及与不共享主干网络进行对比，就是为了给读者更直观的数据来说明算法的有效性

